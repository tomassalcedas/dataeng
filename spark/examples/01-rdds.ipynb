{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucprosa/dataeng-basic-course/blob/main/spark/examples/01-rdds.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_GBE9UsyxwK"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOA_wQSmLd9z"
      },
      "source": [
        "# Data Collections (RDDs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9LeYFsPTjAb"
      },
      "source": [
        "# Setting up PySpark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYXeODL0T1fO",
        "outputId": "0691bf7a-4779-490d-b28a-e14e6470a724"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "%pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "637HFw00T3LP"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master('local').appName('Spark Course').config('spark.ui.port', '4050').getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WW76P6DpLpx"
      },
      "outputs": [],
      "source": [
        "# Diff between SparkSession and SparkContext\n",
        "\n",
        "# both are entry points\n",
        "# SparkSession was introduced in SPark 2.0 - SparkContext was the first/original entry point to Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "SqwZjdL9MnL6"
      },
      "outputs": [],
      "source": [
        "sc = SparkSession.getActiveSession().sparkContext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGG7R76hMZ3E"
      },
      "source": [
        "# RDDs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "DZaxZW0bMZ3F"
      },
      "outputs": [],
      "source": [
        "data = [1, 2, 3, 4, 5]\n",
        "distData = sc.parallelize(data)\n",
        "\n",
        "### Parallelized collections are created by calling SparkContextâ€™s parallelize method on an existing iterable or collection in your driver program\n",
        "### The elements of the collection are copied to form a distributed dataset that can be operated on in parallel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "a8eCChgrMZ3H"
      },
      "outputs": [],
      "source": [
        "### Run parallel operations\n",
        "distData.reduce(lambda a, b: a + b)\n",
        "\n",
        "### One important parameter for parallel collections is the number of partitions to cut the dataset into\n",
        "### Spark will run one task for each partition of the cluster\n",
        "### Typically you want 2-4 partitions for each CPU in your cluster\n",
        "### Normally, Spark tries to set the number of partitions automatically based on your cluster\n",
        "### However, you can also set it manually\n",
        "\n",
        "distData = sc.parallelize(data, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "8UzHNVzynNm-"
      },
      "outputs": [],
      "source": [
        "!mkdir /content/files/\n",
        "text = \"red yellow white green\"\n",
        "\n",
        "text_file = open(\"/content/files/data.txt\", \"w\")\n",
        "text_file.write(text)\n",
        "text_file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "5qs9KNmJMZ3I"
      },
      "outputs": [],
      "source": [
        "### Read from external datasets\n",
        "distFile = sc.textFile(\"/content/files/data.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1ftjIsEMZ3J"
      },
      "outputs": [],
      "source": [
        "### RDDs support two types of operations: transformations, which create a new dataset from an existing one, and actions, which return a value to the driver program after running a computation on the dataset\n",
        "\n",
        "### Example:\n",
        "### MAP is a transformation that passes each dataset element through a function and returns a new RDD representing the results\n",
        "### REDUCE is an action that aggregates all the elements of the RDD using some function and returns the final result to the driver program"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJr1dWvFMZ3J",
        "outputId": "37f79f60-1c1b-4efa-9712-f216e48037af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22\n"
          ]
        }
      ],
      "source": [
        "### All transformations in Spark are lazy!\n",
        "### The transformations are only computed when an action requires a result to be returned to the driver program\n",
        "\n",
        "lines = sc.textFile(\"/content/files/data.txt\")\n",
        "lineLengths = lines.map(lambda s: len(s)) # transformation\n",
        "totalLength = lineLengths.reduce(lambda a, b: a + b) # action\n",
        "print(totalLength)\n",
        "\n",
        "### The first line defines a base RDD from an external file.\n",
        "### lines is merely a pointer to the file\n",
        "### The second line defines lineLengths as the result of a map transformation\n",
        "### lineLengths is not immediately computed, due to laziness\n",
        "### reducer is an actions so Spark breaks the computation into tasks to run on separate machines\n",
        "### and each machine runs both its part of the map and a local reduction, returning only its answer to the driver program."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKflP9NxpweG",
        "outputId": "69fe0c3a-b89e-4c9a-a1fe-84c864f4518d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['red yellow white green']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "lines.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFgcn2llperu",
        "outputId": "5cbffb85-a514-41ba-8e3d-496ed620cb51"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PythonRDD[26] at RDD at PythonRDD.scala:53"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "### if we want to use lineLenghts again\n",
        "lineLengths.persist()\n",
        "### before the reduce, which would cause lineLengths to be saved in memory after the first time it is computed."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}