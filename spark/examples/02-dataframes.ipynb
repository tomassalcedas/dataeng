{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucprosa/dataeng-basic-course/blob/main/spark/examples/02-dataframes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7WZKVZVap7w"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucprosa/dataeng-basic-course/blob/main/spark/examples/02-dataframes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOA_wQSmLd9z"
      },
      "source": [
        "# Dataframe\n",
        "- Dataframes x Datasets\n",
        "- Creating dataframes\n",
        "- StructTypes\n",
        "- Reading data from CSV, JSON and Parquet\n",
        "- Writing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9LeYFsPTjAb"
      },
      "source": [
        "# Setting up PySpark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYXeODL0T1fO"
      },
      "outputs": [],
      "source": [
        "%pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "637HFw00T3LP"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master('local').appName('Spark Course').config('spark.ui.port', '4050').getOrCreate()\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PQL2m7rap75"
      },
      "source": [
        "# DataFrames\n",
        "- Untyped Datasets\n",
        "- Similar to tables in relational databases\n",
        "- DataFrames are just Dataset of Rows in Scala and Java API. These operations are also referred as “untyped transformations” in contrast to “typed transformations” come with strongly typed Scala/Java Datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxrxuq69ap75"
      },
      "outputs": [],
      "source": [
        "# Creating from RDDs\n",
        "# .toDF()\n",
        "\n",
        "lst = [(\"c1\", \"v1\"), (\"c2\", \"v2\"), (\"c3\", \"v3\")]\n",
        "rdd = sc.parallelize(lst)\n",
        "df = rdd.toDF([\"col\", \"value\"])\n",
        "df.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgZYMhj9bA9_"
      },
      "outputs": [],
      "source": [
        "# Using \"createDataFrame\" + StructTypes\n",
        "\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "data = [(\"c1\", \"v1\"), (\"c2\", \"v2\"), (\"c3\", \"v3\")]\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"col\", StringType(), True),\n",
        "    StructField(\"value\", StringType(), True)\n",
        "])\n",
        "\n",
        "df = spark.createDataFrame(data, schema=schema)\n",
        "df.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoaZNT5pcLQt"
      },
      "outputs": [],
      "source": [
        "# Read data from data sources\n",
        "# https://spark.apache.org/docs/3.5.2/sql-data-sources.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdm0B6C1iRb1"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/files/samples/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLSvoAP-eaMh"
      },
      "outputs": [],
      "source": [
        "# from csv\n",
        "import csv\n",
        "\n",
        "# creating csv file\n",
        "with open('/content/files/samples/file.csv', 'w', newline='') as csvfile:\n",
        "    fieldnames = ['col', 'value']\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames, delimiter=\";\")\n",
        "    writer.writeheader()\n",
        "    writer.writerow({'col': 'c1', 'value': 'v1'})\n",
        "    writer.writerow({'col': 'c2', 'value': 'v2'})\n",
        "    writer.writerow({'col': 'c3', 'value': 'v3'})\n",
        "\n",
        "# read csv file\n",
        "df = spark.read.format(\"csv\").load(\"/content/files/samples/file.csv\", sep=\";\", header=True)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Riz_AA40eVz1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# from json\n",
        "json = \"\"\"[{\"col\": \"c1\", \"value\": \"v1\"}, {\"col\": \"c2\", \"value\": \"v2\"}, {\"col\": \"c3\", \"value\": \"v3\"}]\"\"\"\n",
        "\n",
        "text_file = open(\"/content/files/samples/file.json\", \"w\")\n",
        "text_file.write(json)\n",
        "text_file.close()\n",
        "\n",
        "# read from json\n",
        "df = spark.read.json(\"/content/files/samples/file.json\")\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AuQAcN5xeXEx"
      },
      "outputs": [],
      "source": [
        "# from parquet\n",
        "\n",
        "# Generating parquet\n",
        "lst = [(\"c1\", \"v1\"), (\"c2\", \"v2\"), (\"c3\", \"v3\")]\n",
        "rdd = sc.parallelize(lst)\n",
        "df = rdd.toDF([\"col\", \"value\"])\n",
        "df.write.format(\"parquet\").mode(\"overwrite\").save(\"/content/files/samples/parquet\")\n",
        "\n",
        "# read from parquet\n",
        "df2 = spark.read.format(\"parquet\").load(\"/content/files/samples/parquet\")\n",
        "df2.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nWhBQzRjs2F"
      },
      "outputs": [],
      "source": [
        "# Check schema\n",
        "df.printSchema()\n",
        "print(df.schema)\n",
        "print(df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SawfqXMlOAq"
      },
      "outputs": [],
      "source": [
        "# counting items in the dataframe\n",
        "print(df.count())\n",
        "print(df.take(5))\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYbdYYEelZE4"
      },
      "outputs": [],
      "source": [
        "# check explain plan\n",
        "df.explain(\"cost\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5r3u3c6ljyv"
      },
      "outputs": [],
      "source": [
        "df.toJSON().first()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNK9DsAkmKgR"
      },
      "outputs": [],
      "source": [
        "df.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydvdl0epnZ-a"
      },
      "outputs": [],
      "source": [
        "# createOrReplaceTempView\n",
        "df.createOrReplaceTempView(\"my_table\")\n",
        "spark.sql(\"select * from my_table\").show()\n",
        "\n",
        "# createOrReplaceGlobalTempView\n",
        "df.createOrReplaceGlobalTempView(\"my_table2\")\n",
        "spark.sql(\"select * from globaL_temp.my_table2\").show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeZewcPhmnsq"
      },
      "source": [
        "## write operations\n",
        "\n",
        "- df.write.format(\"parquet\").save(path)\n",
        "- df.write.format(\"json\").save(path)\n",
        "- df.write.format(\"csv\").save(path)\n",
        "\n",
        "### if using delta.io\n",
        "- df.write.format(\"delta\").saveAsTable(table_name)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}