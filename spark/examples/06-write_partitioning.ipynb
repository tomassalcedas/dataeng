{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucprosa/dataeng-basic-course/blob/main/spark/examples/06-write_partitioning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOA_wQSmLd9z"
      },
      "source": [
        "# Write\n",
        "- .write\n",
        "- .format (parquet, csv, json)\n",
        "- options\n",
        "- spark.sql.sources.partitionOverwriteMode dynamic\n",
        "\n",
        "# Write Mode\n",
        "- overwrite - The overwrite mode is used to overwrite the existing file, alternatively, you can use SaveMode.Overwrite\n",
        "- append - To add the data to the existing file, alternatively, you can use SaveMode.Append\n",
        "- ignore - Ignores write operation when the file already exists, alternatively, you can use SaveMode.Ignore.\n",
        "- errorifexists or error - This is a default option when the file already exists, it returns an error, alternatively, you can use SaveMode.ErrorIfExists.\n",
        "\n",
        "# Partitioning\n",
        "Process to organize the data into multiple chunks based on some criteria.\n",
        "Partitions are organized in sub-folders.\n",
        "Partitioning improves performance in Spark."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9LeYFsPTjAb"
      },
      "source": [
        "# Setting up PySpark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYXeODL0T1fO"
      },
      "outputs": [],
      "source": [
        "%pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "637HFw00T3LP"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master('local').appName('Spark Course').getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vj3Cg2riVX3m"
      },
      "source": [
        "# Preparing data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faker"
      ],
      "metadata": {
        "id": "83BBHcNJDmw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-caHS2MVX3m"
      },
      "outputs": [],
      "source": [
        "from faker import Faker\n",
        "from datetime import datetime\n",
        "\n",
        "fake = Faker()\n",
        "\n",
        "users = []\n",
        "for _ in range(50):\n",
        "    user = {\n",
        "        'date': fake.date_time_between_dates(datetime(2024, 5, 1), datetime(2024, 5, 5)),\n",
        "        'name': fake.name(),\n",
        "        'address': fake.address(),\n",
        "        'email': fake.email(),\n",
        "        'dob': fake.date_of_birth(),\n",
        "        'phone': fake.phone_number()\n",
        "    }\n",
        "    users.append(user)\n",
        "\n",
        "df = spark.createDataFrame(users)\n",
        "\n",
        "df.show(10, False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGXjf6xpBj36"
      },
      "source": [
        "# Writing as PARQUET\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14stpbb4Bj37"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dw5IIgebBj37"
      },
      "outputs": [],
      "source": [
        "# Writing as PARQUET with no partitions\n",
        "\n",
        "path = \"/content/write_partitioning/parquet_no_partitions\"\n",
        "\n",
        "df.write.mode(\"overwrite\").format(\"parquet\").save(path)\n",
        "\n",
        "!ls /content/write_partitioning/parquet_no_partitions\n",
        "\n",
        "spark.read.format(\"parquet\").load(path).count()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Writing as PARQUET with partitions\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "path = \"/content/write_partitioning/parquet_with_partitions\"\n",
        "\n",
        "# Creating partition column\n",
        "df = df.withColumn(\"date_part\", date_format(col(\"date\"), \"yyyyMMdd\"))\n",
        "\n",
        "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\",\"dynamic\") # enable dynamic partition overwrite - only overwrites partitions that are coming in the dataframe\n",
        "\n",
        "(df#.where(\"date_part = '20240503'\")\n",
        " .write\n",
        " .mode(\"overwrite\")                                               # overwrites the entire path with the new data\n",
        " .partitionBy(\"date_part\")                                        # partition the data by column - creates sub-folders for each partition\n",
        " .format(\"parquet\")                                               # format of output\n",
        " .save(path))                                                     # path\n",
        "\n",
        "!ls /content/write_partitioning/parquet_with_partitions\n",
        "\n",
        "spark.read.format(\"parquet\").load(path).count()"
      ],
      "metadata": {
        "id": "DWX9WZbPHrL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking single partition\n",
        "spark.read.parquet(\"/content/write_partitioning/parquet_with_partitions/date_part=20240502\").show()"
      ],
      "metadata": {
        "id": "0B62qu87JsAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Writing as CSV\n",
        "\n",
        "https://spark.apache.org/docs/3.5.1/sql-data-sources-csv.html"
      ],
      "metadata": {
        "id": "n8mTC5yeNV6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.count()"
      ],
      "metadata": {
        "id": "BnAWUTeZO43Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/write_partitioning/csv_no_partitioning/\"\n",
        "\n",
        "# write as csv\n",
        "(df\n",
        "  .write\n",
        "  .format(\"csv\")\n",
        "  .mode(\"overwrite\")\n",
        "  .option(\"delimiter\", \"|\")\n",
        "  .option(\"header\", True)\n",
        "  .save(path))\n",
        "\n",
        "# listing files in the folder\n",
        "!ls /content/write_partitioning/csv_no_partitioning/\n",
        "\n",
        "# read as csv\n",
        "(spark\n",
        "  .read\n",
        "  .options(sep=\"|\", multiLine=True, header=True)\n",
        "  .csv(path)\n",
        "  .count())"
      ],
      "metadata": {
        "id": "oE6zC-HnNYAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Writing as JSON\n",
        "\n",
        "https://spark.apache.org/docs/3.5.1/sql-data-sources-json.html"
      ],
      "metadata": {
        "id": "ZAuM5-WcTtyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/write_partitioning/json_no_partitioning/\"\n",
        "\n",
        "# write as json\n",
        "(df\n",
        ".write\n",
        ".mode(\"overwrite\")\n",
        ".format(\"json\")\n",
        ".save(path))\n",
        "\n",
        "# listing files in the folder\n",
        "!ls /content/write_partitioning/json_no_partitioning/\n",
        "\n",
        "# read as json\n",
        "(spark\n",
        "  .read\n",
        "  .json(path)\n",
        "  .count())"
      ],
      "metadata": {
        "id": "vnNgwbtxTsW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reading json as text\n",
        "spark.read.text(path).show(10, False)"
      ],
      "metadata": {
        "id": "D3hYNCubT0ry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reading json as text\n",
        "spark.read.json(path).show(10, False)"
      ],
      "metadata": {
        "id": "0bHcT2ilUo_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# partition json data + saveAsTable\n",
        "\n",
        "# Creating partition column\n",
        "df = df.withColumn(\"date_part\", date_format(col(\"date\"), \"yyyyMMdd\"))\n",
        "\n",
        "# write as json\n",
        "(df.write\n",
        "  .partitionBy(\"date_part\")\n",
        "  .mode(\"overwrite\")\n",
        "  .format(\"json\")\n",
        "  .saveAsTable(\"tbl_json_part\"))\n",
        "\n",
        "# read as json\n",
        "spark.table(\"tbl_json_part\").count()\n",
        "\n",
        "# read as json\n",
        "spark.sql(\"show partitions tbl_json_part\").show()"
      ],
      "metadata": {
        "id": "Qj59UNMuU0hV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Append Mode"
      ],
      "metadata": {
        "id": "6RhijzyqZeeq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Writing as PARQUET with APPEND\n",
        "\n",
        "path = \"/content/write_partitioning/parquet_append\"\n",
        "\n",
        "df.write.mode(\"append\").format(\"parquet\").save(path)\n",
        "\n",
        "!ls /content/write_partitioning/parquet_append\n",
        "\n",
        "spark.read.format(\"parquet\").load(path).count()"
      ],
      "metadata": {
        "id": "GmLjA1zDZeG_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}