{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyDay-zKVX3g"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucprosa/dataeng-basic-course/blob/main/spark/examples/10-misc_performance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOA_wQSmLd9z"
      },
      "source": [
        "# Miscellaneos Performance tricks\n",
        "- cache() & persist()\n",
        "- broadcast join\n",
        "- repartition & coalesce\n",
        "- explain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9LeYFsPTjAb"
      },
      "source": [
        "# Setting up PySpark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYXeODL0T1fO"
      },
      "outputs": [],
      "source": [
        "%pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "637HFw00T3LP"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master('local').appName('Spark Course').getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vj3Cg2riVX3m"
      },
      "source": [
        "# Preparing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-caHS2MVX3m"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkFiles\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "# Setting up URLs\n",
        "squirrel_url = \"https://raw.githubusercontent.com/lucprosa/dataeng-basic-course/main/data/squirrel-data/squirrel-data.csv\"\n",
        "park_url = \"https://raw.githubusercontent.com/lucprosa/dataeng-basic-course/main/data/squirrel-data/park-data.csv\"\n",
        "\n",
        "\n",
        "# Defining schemas\n",
        "squirrel_schema = StructType([\n",
        "StructField('Area Name',StringType(),True),\n",
        "StructField('Area ID',StringType(),True),\n",
        "StructField('Park Name',StringType(),True),\n",
        "StructField('Park ID', StringType(), True),\n",
        "StructField('Squirrel ID', StringType(), True),\n",
        "StructField('Primary Fur Color', StringType(), True),\n",
        "StructField('Highlights in Fur Color', StringType(), True),\n",
        "StructField('Color Notes', StringType(), True),\n",
        "StructField('Location', StringType(), True),\n",
        "StructField('Above Ground (Height in Feet)', StringType(), True),\n",
        "StructField('Specific Location', StringType(), True),\n",
        "StructField('Activities', StringType(), True),\n",
        "StructField('Interactions with Humans', StringType(), True),\n",
        "StructField('Squirrel Latitude (DD.DDDDDD)', StringType(), True),\n",
        "StructField('Squirrel Longitude (-DD.DDDDDD)', StringType(), True)\n",
        "])\n",
        "\n",
        "park_schema = StructType([\n",
        "StructField('Area Name',StringType(),True),\n",
        "StructField('Area ID',StringType(),True),\n",
        "StructField('Park Name',StringType(),True),\n",
        "StructField('Park ID', StringType(), True),\n",
        "StructField('Date', StringType(), True),\n",
        "StructField('Start Time', StringType(), True),\n",
        "StructField('End Time', StringType(), True),\n",
        "StructField('Total Time (in minutes, if available)', StringType(), True),\n",
        "StructField('Park Conditions', StringType(), True),\n",
        "StructField('Other Animal Sightings', StringType(), True),\n",
        "StructField('Litter', StringType(), True),\n",
        "StructField('Activities', StringType(), True),\n",
        "StructField('Temperature & Weather', StringType(), True),\n",
        "StructField('Number of Squirrels', IntegerType(), True),\n",
        "StructField('Squirrel Sighter(s)', StringType(), True),\n",
        "StructField('Number of Sighters', IntegerType(), True)\n",
        "])\n",
        "\n",
        "area_schema = StructType([\n",
        "StructField('Area ID',StringType(),True),\n",
        "StructField('Area Name',StringType(),True),\n",
        "StructField('Area Description',StringType(),True),\n",
        "StructField('City Name',StringType(),True),\n",
        "])\n",
        "\n",
        "area_data = [\n",
        "    (\"A\", \"UPPER MANHATTAN\", \"Uptown Manhattan\", \"New York\"),\n",
        "    (\"B\", \"CENTRAL MANHATTAN\", \"Midtown Manhattan\", \"New York\"),\n",
        "    (\"C\", \"LOWER MANHATTAN\", \"Downtown Manhattan\", \"New York\"),\n",
        "    (\"D\", \"BROOKLYN\", \"Brooklyn\", \"New York\")\n",
        "    ]\n",
        "\n",
        "spark.sparkContext.addFile(squirrel_url)\n",
        "spark.sparkContext.addFile(park_url)\n",
        "\n",
        "# creating dataframes\n",
        "squirrel = spark.read.csv(SparkFiles.get(\"squirrel-data.csv\"), header=True, schema=squirrel_schema)\n",
        "park = spark.read.csv(SparkFiles.get(\"park-data.csv\"), header=True, schema=park_schema)\n",
        "area = spark.createDataFrame(data=area_data, schema=area_schema)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1jkoWpweDqW"
      },
      "outputs": [],
      "source": [
        "# show data\n",
        "squirrel.show()\n",
        "park.show()\n",
        "area.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Caching & Persist"
      ],
      "metadata": {
        "id": "9xipEQu1aUcx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Caching\n",
        "# Default: MEMORY_AND_DISK\n",
        "\n",
        "import uuid\n",
        "from pyspark.sql.functions import udf\n",
        "\n",
        "@udf\n",
        "def generate_uuid():\n",
        "  return str(uuid.uuid4())\n",
        "\n",
        "# transformation 1\n",
        "squirrel = squirrel.withColumn(\"hash_id\", generate_uuid())\n",
        "\n",
        "# transformation 2\n",
        "squirrel = squirrel.dropDuplicates()\n",
        "\n",
        "# squirrel.cache().count() <--------------- force an action to run the cache\n",
        "\n",
        "# transformations N\n",
        "# squirrel = squirrel.join...\n",
        "# squirrel = squirrel.groupBy...\n",
        "\n",
        "# DAG\n",
        "# T1 -> T2 -> T3...TN -> A1\n",
        "\n",
        "# action 1\n",
        "# squirrel.write.format(\"parquet\").path(\"path\")\n"
      ],
      "metadata": {
        "id": "mOOh51r6WmDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "squirrel.cache().count()"
      ],
      "metadata": {
        "id": "7vAw8JXIC0it"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "squirrel.is_cached"
      ],
      "metadata": {
        "id": "NTrdLhFlY0NU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yEpZwS_QtJ1"
      },
      "outputs": [],
      "source": [
        "squirrel.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "squirrel.unpersist()"
      ],
      "metadata": {
        "id": "Cyol21etZ4EO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Logical plan: user's code or query and is independent of the underlying data sources and execution strategies\n",
        "\n",
        "- Physical Plan: The physical plan represents the actual execution steps that Spark will perform to execute the job on the cluster."
      ],
      "metadata": {
        "id": "XTjYIv4CF-nm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Persist\n",
        "# Default: MEMORY_ONLY\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import StorageLevel\n",
        "\n",
        "# first execution plan\n",
        "print(area.explain(\"cost\"))\n",
        "print(\"----------------\")\n",
        "\n",
        "area = area.withColumn(\"City shortname\", lit(\"NY\"))\n",
        "# second execution plan\n",
        "print(area.explain(\"cost\"))\n",
        "print(\"----------------\")\n",
        "\n",
        "area = area.persist(StorageLevel.MEMORY_ONLY)\n",
        "area.count()\n",
        "\n",
        "# second execution plan\n",
        "area2 = area.withColumn(\"Teste\", lit(\"test\"))\n",
        "print(area2.explain(\"cost\"))\n",
        "print(\"----------------\")\n",
        "\n",
        "print(area.storageLevel)\n",
        "print(area.is_cached)"
      ],
      "metadata": {
        "id": "cz63P4OGaR3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Persist\n",
        "# Default: MEMORY_AND_DISK\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import StorageLevel\n",
        "\n",
        "# first execution plan\n",
        "print(area.explain(\"cost\"))\n",
        "\n",
        "area = area.withColumn(\"City shortname\", lit(\"NY\"))\n",
        "# second execution plan\n",
        "print(area.explain(\"cost\"))\n",
        "\n",
        "area = area.persist(StorageLevel.DISK_ONLY)\n",
        "area.count()\n",
        "\n",
        "# second execution plan\n",
        "area2 = area.withColumn(\"Teste\", lit(\"test\"))\n",
        "print(area2.explain(\"cost\"))\n",
        "\n",
        "print(area.storageLevel)\n",
        "print(area.is_cached)"
      ],
      "metadata": {
        "id": "Y8lWwqd4bK5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Broadcast Join"
      ],
      "metadata": {
        "id": "jG6-tBXIocwm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tshz4hV3S2Tx"
      },
      "outputs": [],
      "source": [
        "# Broadcast join\n",
        "# identify the tables candidates for broadcast (smaller one)\n",
        "\n",
        "join_df = (squirrel\n",
        "           .join(park, on=\"Park ID\", how=\"inner\")\n",
        "           .join(area, on=\"Area ID\", how=\"inner\")\n",
        "           .select(area[\"Area Description\"], park[\"Park Name\"], park[\"Date\"], squirrel[\"Squirrel ID\"])\n",
        "           )\n",
        "\n",
        "join_df.explain()\n",
        "join_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Repartition & Coalesce\n",
        "\n",
        "- coalesce is for reducing partitions without shuffling\n",
        "- repartition is for distributing data evenly across the cluster for better parallelism\n",
        "\n",
        "- if possible choose coalesce over repartition\n",
        "- if needed to increase partitions to increase parallelism, use repartition, however keep the data shuffling operation in mind\n",
        "\n"
      ],
      "metadata": {
        "id": "K5wkJmgQs8UX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "squirrel_1 = squirrel\n",
        "squirrel_2 = squirrel\n",
        "\n",
        "# Check partitions\n",
        "squirrel_1.rdd.getNumPartitions()\n",
        "\n",
        "# RDD -> partitions among the workers"
      ],
      "metadata": {
        "id": "EitF3d_es8Fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# repartition\n",
        "# evenly distribute date across partitions for better parallel processing efficiency\n",
        "# increase AND reduce partitions\n",
        "# do shuffling\n",
        "\n",
        "print(f\"before repartition: {squirrel_1.rdd.getNumPartitions()}\")\n",
        "squirrel_1 = squirrel_1.repartition(4)\n",
        "print(f\"after repartition: {squirrel_1.rdd.getNumPartitions()}\")"
      ],
      "metadata": {
        "id": "wKBD2qXCtzwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# coalesce\n",
        "# reduce partitions without shuffling\n",
        "# minimizes data movement across the cluster\n",
        "\n",
        "# does not allow to increase partitions, only reduce\n",
        "print(f\"before coalesce: {squirrel_2.rdd.getNumPartitions()}\")\n",
        "squirrel_2 = squirrel_2.coalesce(5)\n",
        "print(f\"after coalesce: {squirrel_2.rdd.getNumPartitions()}\")\n"
      ],
      "metadata": {
        "id": "72Mk2UJwLEKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"before coalesce: {squirrel_1.rdd.getNumPartitions()}\")\n",
        "squirrel_1 = squirrel_1.coalesce(2)\n",
        "print(f\"after coalesce: {squirrel_1.rdd.getNumPartitions()}\")"
      ],
      "metadata": {
        "id": "6ER9DqONMKE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# repartition/coalesce and writing data\n",
        "!rm -rf /content/files/area\n",
        "!mkdir -p /content/files/area\n",
        "\n",
        "# repartition \"area\" dataframe and write as parquet\n",
        "area.repartition(3).write.format(\"parquet\").mode(\"overwrite\").save(\"/content/files/area\")"
      ],
      "metadata": {
        "id": "IR5COTOyPT_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check files and their content\n",
        "\n",
        "files = !ls /content/files/area/ | grep \".parquet\"\n",
        "folder = \"/content/files/area/\"\n",
        "\n",
        "for f in files:\n",
        "  df = spark.read.parquet(f\"{folder}{f}\")\n",
        "  print(f\"{f} - {df.count()} rows\")"
      ],
      "metadata": {
        "id": "bl3NMEx6PhYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check file sizes\n",
        "!ls /content/files/area"
      ],
      "metadata": {
        "id": "n5H-rpwqQS_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question"
      ],
      "metadata": {
        "id": "aVyWuRHiR--p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q1\n",
        "# read data from /content/files/area (3 parquet files)\n",
        "# write again the data into the same folder making sure the output will be only one file"
      ],
      "metadata": {
        "id": "hLjzGtv-Rou1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}