{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucprosa/dataeng-basic-course/blob/main/spark_streaming/examples/example_3_api_json.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_GBE9UsyxwK"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9LeYFsPTjAb"
      },
      "source": [
        "# Setting up PySpark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYXeODL0T1fO",
        "outputId": "b805aca4-2d12-47de-d985-2b8a22eeb565"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "%pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "637HFw00T3LP"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.master('local').appName('Test streaming').getOrCreate()\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/landing\n",
        "!rm -rf /content/bronze\n",
        "!mkdir -p /content/landing"
      ],
      "metadata": {
        "id": "aF7fzyYIJi0l"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simulate producer:\n",
        "- extract data from API\n",
        "- store data as json in the lake\n",
        "- run task async"
      ],
      "metadata": {
        "id": "RZdHGoFyTlMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from pyspark.sql.types import *\n",
        "import json\n",
        "import datetime\n",
        "import asyncio\n",
        "\n",
        "async def ingest_from_api(url: str, table: str, schema: StructType = None):\n",
        "  response = requests.get(url)\n",
        "  timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "  if response.status_code == 200:\n",
        "    data = response.json()\n",
        "    with open(f\"/content/landing/{table}_{int(timestamp)}.json\", \"w\") as f:\n",
        "        json.dump(data, f)\n",
        "\n",
        "async def producer(loop: int, interval_time: int):\n",
        "  for i in range(loop):\n",
        "    await ingest_from_api(\"https://api.carrismetropolitana.pt/vehicles\", \"vehicles\")\n",
        "    await ingest_from_api(\"https://api.carrismetropolitana.pt/lines\", \"lines\")\n",
        "    await asyncio.sleep(interval_time)\n",
        "\n",
        "async def main():\n",
        "  asyncio.create_task(producer(10, 30))\n",
        "\n",
        "await main()"
      ],
      "metadata": {
        "id": "tTQhp8UbFUCl"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Read from /content/landing as streaming\n",
        "- store data in memory (for testing)\n",
        "- store data in the bronze layer"
      ],
      "metadata": {
        "id": "kIqHdZEKUEmo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "\n",
        "vehicle_schema = StructType([StructField('bearing', IntegerType(), True),\n",
        "                             StructField('block_id', StringType(), True),\n",
        "                             StructField('current_status', StringType(), True),\n",
        "                             StructField('id', StringType(), True),\n",
        "                             StructField('lat', FloatType(), True),\n",
        "                             StructField('line_id', StringType(), True),\n",
        "                             StructField('lon', FloatType(), True),\n",
        "                             StructField('pattern_id', StringType(), True),\n",
        "                             StructField('route_id', StringType(), True),\n",
        "                             StructField('schedule_relationship', StringType(), True),\n",
        "                             StructField('shift_id', StringType(), True),\n",
        "                             StructField('speed', FloatType(), True),\n",
        "                             StructField('stop_id', StringType(), True),\n",
        "                             StructField('timestamp', TimestampType(), True),\n",
        "                             StructField('trip_id', StringType(), True)])\n",
        "\n",
        "stream = spark.readStream.format(\"json\").schema(vehicle_schema).load(\"/content/landing/vehicles*\")\n",
        "\n",
        "dedup = stream.dropDuplicates()"
      ],
      "metadata": {
        "id": "_dTSf527Fhy0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using memory for testing\n",
        "try:\n",
        "  if query.isActive:\n",
        "    query.stop()\n",
        "except:\n",
        "  pass\n",
        "\n",
        "query = (dedup.writeStream.format(\"memory\").option(\"queryName\", \"vehicles\").start())"
      ],
      "metadata": {
        "id": "9N99eI41UUFA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"select * from vehicles\").show()"
      ],
      "metadata": {
        "id": "wT9pNrwoXBi4",
        "outputId": "e7965d3a-93cd-47e0-e916-8c4a92efc38c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------------+--------------+--------+---------+-------+---------+----------+--------+---------------------+------------+---------+-------+-------------------+--------------------+\n",
            "|bearing|            block_id|current_status|      id|      lat|line_id|      lon|pattern_id|route_id|schedule_relationship|    shift_id|    speed|stop_id|          timestamp|             trip_id|\n",
            "+-------+--------------------+--------------+--------+---------+-------+---------+----------+--------+---------------------+------------+---------+-------+-------------------+--------------------+\n",
            "|    209|       ESC_DU_EU2028|   INCOMING_AT| 43|2236|38.617523|   3012|-9.193602|  3012_0_2|  3012_0|            SCHEDULED|      EU2230|      7.5| 020417|2024-11-29 00:15:31|3012_0_2_2400_242...|\n",
            "|      0|                1060|    STOPPED_AT| 42|1060| 38.83711|   2751|-9.187792|  2751_1_2|  2751_1|            SCHEDULED|       39743|      0.0| 070021|2024-11-29 00:16:12|2751_1_2|140|1|23...|\n",
            "|    120|       ESC_DU_EU2012| IN_TRANSIT_TO| 43|2227|38.653088|   3025|-9.215877|  3025_0_1|  3025_0|            SCHEDULED|      EU2231|13.055555| 020397|2024-11-29 00:16:15|3025_0_1_2400_242...|\n",
            "|    249|           1_1341-11|   INCOMING_AT| 41|1180| 38.68752|   1622|-9.335765|  1622_0_1|  1622_0|            SCHEDULED|        1362|      0.0| 050009|2024-11-29 00:14:29|1622_0_1_2330_235...|\n",
            "|    339|UNAVAILABLE_BLOCK_ID| IN_TRANSIT_TO| 41|1372|38.762882|   1518|-9.260711|  1518_0_2|  1518_0|            SCHEDULED|        1694|5.5555553| 170893|2024-11-29 00:15:28|1518_0_2_2330_235...|\n",
            "|    328|             1096-11|   INCOMING_AT| 42|2416|38.861526|   2538|-9.122987|  2538_0_1|  2538_0|            SCHEDULED|        1276|12.777778| 070925|2024-11-29 00:16:13|2538_0_1|1|1|2355...|\n",
            "|    174|             1080-11| IN_TRANSIT_TO|  42|263|38.892998|   2303|-9.035446|  2303_0_2|  2303_0|            SCHEDULED|        1206|      5.0| 180041|2024-11-29 00:15:44|2303_0_2|1|1|2400...|\n",
            "|      5|20241128-64010569...|    STOPPED_AT|44|12686|38.768215|   4705|-9.101175|  4705_0_1|  4705_0|            SCHEDULED|123530234560|      0.0| 060002|2024-11-29 00:15:46|4705_0_1|2800|243...|\n",
            "|    138|UNAVAILABLE_BLOCK_ID| IN_TRANSIT_TO| 42|2317| 38.79801|   2713|-9.106971|  2713_0_3|  2713_0|            SCHEDULED|        1208|12.777778| 071295|2024-11-29 00:15:48|2713_0_3|1|1|2345...|\n",
            "|    230|20241128-64010153...|   INCOMING_AT|44|12679| 38.52968|   4715|-8.881378|  4715_0_1|  4715_0|            SCHEDULED|113280234560|10.555555| 160029|2024-11-29 00:16:18|4715_0_1|2800|233...|\n",
            "|    137|20241128-64010052...| IN_TRANSIT_TO|44|12501|38.717796|   4600| -8.99687|  4600_0_1|  4600_0|            SCHEDULED|121910234560|     12.5| 100285|2024-11-29 00:16:39|4600_0_1|2800|240...|\n",
            "|     93|       ESC_DU_EU2058| IN_TRANSIT_TO| 43|2123|38.706295|   3708|-9.144389|  3708_0_1|  3708_0|            SCHEDULED|      EU2207|      0.0| 060419|2024-11-29 00:16:25|3708_0_1_2300_232...|\n",
            "|    151|             1046-11| IN_TRANSIT_TO| 42|2319|38.768906|   2703| -9.15879|  2703_0_3|  2703_0|            SCHEDULED|        1203|7.2222223| 060169|2024-11-29 00:16:29|2703_0_3|1|1|2340...|\n",
            "|      0|             1809-11|   INCOMING_AT| 42|2577|38.813595|   2769|-9.159491|  2769_1_2|  2769_1|            SCHEDULED|        1840|3.8888888| 071267|2024-11-29 00:15:38|2769_1_2|1|1|2400...|\n",
            "|    167|       ESC_DU_EU3010| IN_TRANSIT_TO|  43|671| 38.48291|   3545|-9.104219|  3545_0_1|  3545_0|            SCHEDULED|      EU3094|      0.0| 150219|2024-11-29 00:16:33|3545_0_1_2330_235...|\n",
            "|     65|             1579-11| IN_TRANSIT_TO| 42|2538|38.759285|   2812|-9.159841|  2812_1_2|  2812_1|            SCHEDULED|        1661| 6.111111| 060157|2024-11-29 00:16:48|2812_1_2|1|1|2415...|\n",
            "|    332|           1_1023-11|   INCOMING_AT| 41|1876| 38.73717|   1502|-9.227831|  1502_0_1|  1502_0|            SCHEDULED|        1139|     20.0| 030533|2024-11-29 00:15:45|1502_0_1_2330_235...|\n",
            "|    158|             1046-11| IN_TRANSIT_TO| 42|2319| 38.77094|   2703|-9.160124|  2703_0_3|  2703_0|            SCHEDULED|        1203|6.6666665| 060169|2024-11-29 00:15:33|2703_0_3|1|1|2340...|\n",
            "|     40|           1_1465-11|   INCOMING_AT| 41|1143|38.716614|   1728|-9.253231|  1728_1_2|  1728_1|            SCHEDULED|        1506|     15.0| 120288|2024-11-29 00:16:15|1728_1_2_2400_242...|\n",
            "|    300|       ESC_DU_EU2028| IN_TRANSIT_TO| 43|2236|38.619457|   3012| -9.19678|  3012_0_2|  3012_0|            SCHEDULED|      EU2230|13.055555| 020413|2024-11-29 00:16:38|3012_0_2_2400_242...|\n",
            "+-------+--------------------+--------------+--------+---------+-------+---------+----------+--------+---------------------+------------+---------+-------+-------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/bronze"
      ],
      "metadata": {
        "id": "W1BCl7BCXo_v"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *\n",
        "\n",
        "# watermark is necessary because of the aggregation\n",
        "transformed = stream.withWatermark(\"timestamp\", \"60 seconds\")\n",
        "agg = (transformed\n",
        "       .groupBy(window(transformed.timestamp, \"5 minutes\"), col(\"current_status\"))\n",
        "       .agg(min(transformed.timestamp).alias(\"init_timestamp\"), count(\"*\").alias(\"count\")))\n",
        "\n",
        "def insert_vehicles(df, batch_id):\n",
        "  #df2 = df.groupBy(\"window\").pivot(\"current_status\").sum(\"count\")\n",
        "  df.write.format(\"parquet\").mode(\"append\").save(\"/content/bronze/vehicles\")\n",
        "\n",
        "# using memory for testing\n",
        "query2 = (agg\n",
        "          .writeStream\n",
        "          .outputMode(\"append\")\n",
        "          .foreachBatch(insert_vehicles)\n",
        "          .option(\"checkpointLocation\", \"/content/bronze/checkpoint\")\n",
        "          .trigger(processingTime='20 seconds')\n",
        "          .start())"
      ],
      "metadata": {
        "id": "xyDkRdgLUZZt"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.read.format(\"parquet\").load(\"/content/bronze/vehicles/*\").show(100, False)"
      ],
      "metadata": {
        "id": "d6xqFWyKdujI",
        "outputId": "78794dc5-db08-468b-c5b2-3ad101451d29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------+--------------+-------------------+-----+\n",
            "|window                                    |current_status|init_timestamp     |count|\n",
            "+------------------------------------------+--------------+-------------------+-----+\n",
            "|{2024-11-29 00:25:00, 2024-11-29 00:30:00}|IN_TRANSIT_TO |2024-11-29 00:29:47|5    |\n",
            "|{2024-11-29 00:10:00, 2024-11-29 00:15:00}|IN_TRANSIT_TO |2024-11-29 00:14:31|18   |\n",
            "|{2024-11-29 00:15:00, 2024-11-29 00:20:00}|IN_TRANSIT_TO |2024-11-29 00:15:02|916  |\n",
            "|{2024-11-29 00:20:00, 2024-11-29 00:25:00}|IN_TRANSIT_TO |2024-11-29 00:20:00|2    |\n",
            "|{2024-11-29 00:15:00, 2024-11-29 00:20:00}|INCOMING_AT   |2024-11-29 00:15:01|308  |\n",
            "|{2024-11-29 00:25:00, 2024-11-29 00:30:00}|INCOMING_AT   |2024-11-29 00:29:13|6    |\n",
            "|{2024-11-29 00:10:00, 2024-11-29 00:15:00}|INCOMING_AT   |2024-11-29 00:14:29|4    |\n",
            "|{2024-11-29 00:20:00, 2024-11-29 00:25:00}|INCOMING_AT   |2024-11-29 00:20:00|2    |\n",
            "|{2024-11-29 00:15:00, 2024-11-29 00:20:00}|STOPPED_AT    |2024-11-29 00:15:00|295  |\n",
            "|{2024-11-29 00:10:00, 2024-11-29 00:15:00}|STOPPED_AT    |2024-11-29 00:14:31|16   |\n",
            "|{2024-11-29 00:25:00, 2024-11-29 00:30:00}|STOPPED_AT    |2024-11-29 00:29:12|10   |\n",
            "+------------------------------------------+--------------+-------------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Report\n",
        "- show vehicles by status in 5-min window time\n",
        "- one line per window time"
      ],
      "metadata": {
        "id": "62oGSmx4S8Ch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pivot_data(df: DataFrame):\n",
        "  result = df.orderBy(\"init_timestamp\").groupBy(\"window\").pivot(\"current_status\").sum(\"count\")\n",
        "  result.show(100, False)\n",
        "\n",
        "df = spark.read.format(\"parquet\").load(\"/content/bronze/vehicles/*\")\n",
        "pivot_data(df)"
      ],
      "metadata": {
        "id": "x38lvoysfKLy",
        "outputId": "a061dcff-401d-49da-c906-55f5aef99e1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------+-----------+-------------+----------+\n",
            "|window                                    |INCOMING_AT|IN_TRANSIT_TO|STOPPED_AT|\n",
            "+------------------------------------------+-----------+-------------+----------+\n",
            "|{2024-11-29 00:20:00, 2024-11-29 00:25:00}|2          |2            |NULL      |\n",
            "|{2024-11-29 00:15:00, 2024-11-29 00:20:00}|308        |916          |295       |\n",
            "|{2024-11-29 00:10:00, 2024-11-29 00:15:00}|4          |18           |16        |\n",
            "|{2024-11-29 00:25:00, 2024-11-29 00:30:00}|6          |5            |10        |\n",
            "+------------------------------------------+-----------+-------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query2.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyDwzbdmO29f",
        "outputId": "ff21e3ee-6769-4a80-de42-7bc10cc5ddc5"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:py4j.clientserver:There was an exception while executing the Python Proxy on the Python Side.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 617, in _call_proxy\n",
            "    return_value = getattr(self.pool[obj_id], method)(*params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/sql/utils.py\", line 120, in call\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/sql/utils.py\", line 117, in call\n",
            "    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)\n",
            "  File \"<ipython-input-48-9446b2336b0e>\", line 11, in insert_vehicles\n",
            "    df.write.format(\"parquet\").mode(\"append\").save(\"/content/bronze/vehicles\")\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/sql/readwriter.py\", line 1463, in save\n",
            "    self._jwrite.save(path)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
            "    return_value = get_return_value(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
            "    return f(*a, **kw)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/protocol.py\", line 326, in get_return_value\n",
            "    raise Py4JJavaError(\n",
            "py4j.protocol.Py4JJavaError: An error occurred while calling o457.save.\n",
            ": java.lang.InterruptedException\n",
            "\tat java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1040)\n",
            "\tat java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1345)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:242)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:258)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:187)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitReady(ThreadUtils.scala:342)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:980)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
            "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
            "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
            "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
            "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
            "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
            "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
            "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
            "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
            "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
            "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
            "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
            "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
            "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
            "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
            "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
            "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
            "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
            "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
            "\tat com.sun.proxy.$Proxy35.call(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)\n",
            "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)\n",
            "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3ETfknUmUeZg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}