{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/lucprosa/dataeng-basic-course/blob/main/spark_streaming/challenges/final_challenges.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_GBE9UsyxwK"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d9LeYFsPTjAb"
   },
   "source": [
    "# Setting up PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uYXeODL0T1fO",
    "outputId": "b805aca4-2d12-47de-d985-2b8a22eeb565"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rcybt71kTDNt"
   },
   "source": [
    "# Context\n",
    "Message events are coming from platform message broker (kafka, pubsub, kinesis...).\n",
    "You need to process the data according to the requirements.\n",
    "\n",
    "Message schema:\n",
    "- timestamp\n",
    "- value\n",
    "- event_type\n",
    "- message_id\n",
    "- country_id\n",
    "- user_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JkyPORKNSYvV"
   },
   "source": [
    "# Challenge 1\n",
    "\n",
    "Step 1\n",
    "- Change exising producer\n",
    "\t- Change parquet location to \"/content/lake/bronze/messages/data\"\n",
    "\t- Add checkpoint (/content/lake/bronze/messages/checkpoint)\n",
    "\t- Delete /content/lake/bronze/messages and reprocess data\n",
    "\t- For reprocessing, run the streaming for at least 1 minute, then stop it\n",
    "\n",
    "Step 2\n",
    "- Implement new stream job to read from messages in bronze layer and split result in two locations\n",
    "\t- \"messages_corrupted\"\n",
    "\t\t- logic: event_status is null, empty or equal to \"NONE\"\n",
    "    - extra logic: add country name by joining message with countries dataset\n",
    "\t\t- partition by \"date\" -extract it from timestamp\n",
    "\t\t- location: /content/lake/silver/messages_corrupted/data\n",
    "\n",
    "\t- \"messages\"\n",
    "\t\t- logic: not corrupted data\n",
    "\t\t- extra logic: add country name by joining message with countries dataset\n",
    "\t\t- partition by \"date\" -extract it from timestamp\n",
    "\t\t- location: /content/lake/silver/messages/data\n",
    "\n",
    "\t- technical requirements\n",
    "\t\t- add checkpint (choose location)\n",
    "\t\t- use StructSchema\n",
    "\t\t- Set trigger interval to 5 seconds\n",
    "\t\t- run streaming for at least 20 seconds, then stop it\n",
    "\n",
    "\t- alternatives\n",
    "\t\t- implementing single streaming job with foreach/- foreachBatch logic to write into two locations\n",
    "\t\t- implementing two streaming jobs, one for messages and another for messages_corrupted\n",
    "\t\t- (paying attention on the paths and checkpoints)\n",
    "\n",
    "\n",
    "  - Check results:\n",
    "    - results from messages in bronze layer should match with the sum of messages+messages_corrupted in the silver layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Udk3tohSaXOH",
    "outputId": "de9b696a-aceb-42c6-ff45-1bab77511796"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faker in /usr/local/lib/python3.10/dist-packages (33.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.10/dist-packages (from faker) (2.8.2)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from faker) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.4->faker) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "%pip install faker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cDGMKwBdi1qy"
   },
   "source": [
    "# Producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tPCOdivrfhYh",
    "outputId": "d64c5b30-ab8c-4fdb-e5bf-7eeeea967874"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "from faker import Faker\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('Test streaming').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "fake = Faker()\n",
    "messages = [fake.uuid4() for _ in range(50)]\n",
    "\n",
    "def enrich_data(df, messages=messages):\n",
    "  fake = Faker()\n",
    "  new_columns = {\n",
    "      'event_type': F.lit(fake.random_element(elements=('OPEN', 'RECEIVED', 'SENT', 'CREATED', 'CLICKED', '', 'NONE'))),\n",
    "      'message_id': F.lit(fake.random_element(elements=messages)),\n",
    "      'channel': F.lit(fake.random_element(elements=('CHAT', 'EMAIL', 'SMS', 'PUSH', 'OTHER'))),\n",
    "      'country_id': F.lit(fake.random_int(min=2000, max=2015)),\n",
    "      'user_id': F.lit(fake.random_int(min=1000, max=1050)),\n",
    "  }\n",
    "  df = df.withColumns(new_columns)\n",
    "  return df\n",
    "\n",
    "def insert_messages(df: DataFrame, batch_id):\n",
    "  enrich = enrich_data(df)\n",
    "  enrich.write.mode(\"append\").format(\"parquet\").save(\"content/lake/bronze/messages\")\n",
    "\n",
    "# read stream\n",
    "df_stream = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 1).load()\n",
    "\n",
    "# write stream\n",
    "query = (df_stream.writeStream\n",
    ".outputMode('append')\n",
    ".trigger(processingTime='1 seconds')\n",
    ".foreachBatch(insert_messages)\n",
    ".start()\n",
    ")\n",
    "\n",
    "query.awaitTermination(60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KNyUK3yplDhg"
   },
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZWQExsnzlMFe"
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"parquet\").load(\"content/lake/bronze/messages/*\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RraxHCycMdEZ"
   },
   "source": [
    "# Additional datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cfsus3dxMcQI"
   },
   "outputs": [],
   "source": [
    "countries = [\n",
    "    {\"country_id\": 2000, \"country\": \"Brazil\"},\n",
    "    {\"country_id\": 2001, \"country\": \"Portugal\"},\n",
    "    {\"country_id\": 2002, \"country\": \"Spain\"},\n",
    "    {\"country_id\": 2003, \"country\": \"Germany\"},\n",
    "    {\"country_id\": 2004, \"country\": \"France\"},\n",
    "    {\"country_id\": 2005, \"country\": \"Italy\"},\n",
    "    {\"country_id\": 2006, \"country\": \"United Kingdom\"},\n",
    "    {\"country_id\": 2007, \"country\": \"United States\"},\n",
    "    {\"country_id\": 2008, \"country\": \"Canada\"},\n",
    "    {\"country_id\": 2009, \"country\": \"Australia\"},\n",
    "    {\"country_id\": 2010, \"country\": \"Japan\"},\n",
    "    {\"country_id\": 2011, \"country\": \"China\"},\n",
    "    {\"country_id\": 2012, \"country\": \"India\"},\n",
    "    {\"country_id\": 2013, \"country\": \"South Korea\"},\n",
    "    {\"country_id\": 2014, \"country\": \"Russia\"},\n",
    "    {\"country_id\": 2015, \"country\": \"Argentina\"}\n",
    "]\n",
    "\n",
    "countries = spark.createDataFrame(countries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pg2nx03_Sn62"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swvPj9hVpzNf"
   },
   "source": [
    "# Streaming Messages x Messages Corrupted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZAHIZeZMlpoH",
    "outputId": "112472fb-eda9-4fb0-e7f4-365f08940886"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jLK9jpjCu3xE"
   },
   "source": [
    "## Checking data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nk8seEvbmvcU"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfxIlBISSvRP"
   },
   "source": [
    "# Challenge 2\n",
    "\n",
    "- Run business report\n",
    "- But first, there is a bug in the system which is causing some duplicated messages, we need to exclude these lines from the report\n",
    "\n",
    "- removing duplicates logic:\n",
    "  - Identify possible duplicates on message_id, event_type and channel\n",
    "  - in case of duplicates, consider only the first message (occurrence by timestamp)\n",
    "  - Ex:\n",
    "    In table below, the correct message to consider is the second line\n",
    "\n",
    "```\n",
    "    message_id | channel | event_type | timestamp\n",
    "    123        | CHAT    | CREATED    | 10:10:01\n",
    "    123        | CHAT    | CREATED    | 07:56:45 (first occurrence)\n",
    "    123        | CHAT    | CREATED    | 08:13:33\n",
    "```\n",
    "\n",
    "- After cleaning the data we're able to create the busines report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R3J9XyOHhqvU"
   },
   "outputs": [],
   "source": [
    "# dedup data\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "df = spark.read.format(\"parquet\").load(\"content/lake/silver/messages\")\n",
    "dedup = df.withColumn(\"row_number\", F.row_number().over(Window.partitionBy(\"message_id\", \"event_type\", \"channel\").orderBy(\"timestamp\"))).filter(\"row_number = 1\").drop(\"row_number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RF9L9i25lk74"
   },
   "source": [
    "### Report 1\n",
    "  - Aggregate data by date, event_type and channel\n",
    "  - Count number of messages\n",
    "  - pivot event_type from rows into columns\n",
    "  - schema expected:\n",
    "  \n",
    "```\n",
    "|      date|channel|CLICKED|CREATED|OPEN|RECEIVED|SENT|\n",
    "+----------+-------+-------+-------+----+--------+----+\n",
    "|2024-12-03|    SMS|      4|      4|   1|       1|   5|\n",
    "|2024-12-03|   CHAT|      3|      7|   5|       8|   4|\n",
    "|2024-12-03|   PUSH|   NULL|      3|   4|       3|   4|\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UPHSMSXnTKgu"
   },
   "outputs": [],
   "source": [
    "# report 1\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bxwOawo2lwQH"
   },
   "source": [
    "## Report 2\n",
    "\n",
    "- Identify the most active users by channel (sorted by number of iterations)\n",
    "- schema expected:\n",
    "\n",
    "```\n",
    "+-------+----------+----+-----+-----+----+---+\n",
    "|user_id|iterations|CHAT|EMAIL|OTHER|PUSH|SMS|\n",
    "+-------+----------+----+-----+-----+----+---+\n",
    "|   1022|         5|   2|    0|    1|   0|  2|\n",
    "|   1004|         4|   1|    1|    1|   1|  0|\n",
    "|   1013|         4|   0|    0|    2|   1|  1|\n",
    "|   1020|         4|   2|    0|    1|   1|  0|\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rsS7bkAJmWsW"
   },
   "outputs": [],
   "source": [
    "# report 2\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9_kzDbDwDOS"
   },
   "source": [
    "# Challenge 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ef0RjFTxwE5y"
   },
   "outputs": [],
   "source": [
    "# Theoretical question:\n",
    "\n",
    "# A new usecase requires the message data to be aggregate in near real time\n",
    "# They want to build a dashboard embedded in the platform website to analyze message data in low latency (few minutes)\n",
    "# This application will access directly the data aggregated by streaming process\n",
    "\n",
    "# Q1:\n",
    "- What would be your suggestion to achieve that using Spark Structure Streaming?\n",
    "Or would you choose a different data processing tool?\n",
    "\n",
    "- Which storage would you use and why? (database?, data lake?, kafka?)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
